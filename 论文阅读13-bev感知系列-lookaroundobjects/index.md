# 论文阅读13 BEV感知系列-LookAroundObjects



本文是了解BEV感知系列的第一篇论文阅读，提出了前景运动遮挡对构建语义Map的影响，并提出了一种hallucination背景深度和语义的方法，从而避免了大量的手工标注。

<!--more-->


## 简介

-   论文：《Learning to Look around Objects for Top-View Representations of Outdoor Scenes》
-   作者：Samuel Schulter，Menghua Zhai2，
Nathan Jacobs2，Manmohan Chandraker
-   机构：_NEC-Labs, Computer Science University of Kentucky, UC San Diego_
-   论文水平：_ECCV 2018_
-   关键词：**HallucinatingOcclusion && BEV segmentation**
-   论文链接：[paper](https://arxiv.org/abs/1803.10870)  



## TL;DR

[总结与思考](#总结与思考)

## 摘要

给定透视图中复杂户外道路场景的单个 RGB 图像，我们要解决在BEV视图中估计遮挡语义场景布局的问题。这个具有挑战性的问题不仅需要准确理解 3D 几何和可见场景的语义，还需要准确理解遮挡区域。

我们提出了一个卷积神经网络，它通过观察汽车或行人等前景物体来学习预测场景布局的遮挡部分。但是，我们展示了直接预测遮挡区域的语义和深度，而不是产生幻觉 RGB 值，可以更好地转换到BEV视图。

实验进一步表明，通过从模拟数据或（如果可用）地图数据中学习有关典型道路布局的先验和规则，可以显着增强这种BEV视图表示。更重要的是，训练我们的模型不需要对遮挡区域或顶视图进行昂贵或主观的人工注释，而是使用现成的注释进行标准语义分割。我们在 KITTI 和 Cityscapes 数据集上评估了我们的方法。

## 主要贡献

1. 对于BEV视角下的遮挡预测问题，提出了一种针对语义和深度预测的方案，进而并不需要手工标注BEV下的遮挡区域。此外，转bev的过程也是通过深度预测实现。
2. 提出了VPN网络，有效地对输入前向视图做变换以及多视角的融合。
3. 在KITTI于Cityscapes数据集上均做了验证，并且证明使用模拟或地图先验会显著提升模型的性能。

## 方法框架

### 问题定义

将RGB透视视角变换到BEV视角，而BEV视角往往存在遮挡，因此如何处理遮挡就成了重要问题。本文的算法输入是一张透视视角的图像，以及前景分割好的语义图像，前景即为可视区域。相对应地，将汽车与行人作为遮挡物。

估计带有遮挡的BEV的一个直观重要的步骤就是推理出前景背后的语义和几何信息。

### pipeline


![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220421113631.png)

#### 输入

上图a是网络的大概结构， 网络的输入是两部分：
- 首先是mask，这里的mask指的是对前景类别的语义分割结果，shape为HxWxC，C是前景类别数量。
- 而Masked RGB则是对原图的掩码处理，这个mask的概念是，如果图像中的像素属于前景，那么mask值为1，否则为0，然后根据mask的值对RGB进行处理，如果mask等于1，那么RGB中的像素值替换为所有像素的均值。显然，如果图像经过了中心化，那么RGB上做掩码的值应该为0。

#### encoder

上述两部分输入均经过CNN进行处理，Masked RGB被送到inpainting CNN的网络中，而编码类别的mask则使用一个小的CNN结构进行编码。

#### decoder

解码器部分，首先将两分支的编码做级联，然后使用两分支解码器输出不同的结果。对于语义分割而言，使用的仍然是PSP中的网络，利用融合特征预测语义信息。深度预测部分，使用了先前的网络结构预测深度。这两部分解码器均需要通过双线性插值，使feature map调整到原图大小。

值得注意的是，编码器的部分对前景目标进行了掩码，然后利用 学习到的特征做深度解码，这样子等同于在输入部分将前景信息盖住，然后将背景以及替换前景的像素交给网络，让网络预测前景后面所遮挡的深度以及语义信息。这是较为具有创新性的。

#### training

训练部分，通过有监督信息进行训练，监督信号则是语义分割的label以及深度的真值，而在真实场景下前景目标的深度信息无法获取，因此本工作**只计算背景的深度loss，前景忽略**。此外，还将输入的masked rgb做了额外的随机掩码，通过随机生成的掩码块将图像随机遮盖住，这样可以让网络学习到遮挡区域的深度信息和语义信息。

#### mapping into BEV

通过生成的深度图以及相机内参矩阵K，完成向BEV视角的投影，丢弃了z轴高度的信息，直接生成128x64的BEV图像，对应60x30m的真实场景。对于多个点映射到同一个像素中的情况，通过对类别分布进行平均。最终得到Binit。

这样简单的预测深度并且向BEV投影，误差会很多，论文中作者也提到了这个问题，会存在一些像素在各个类别的概率都很低的情况，以及像素深度错误估计的情况。为了进一步优化，作者提出了一个refinement CNN网络，其将上述的Binit作为输入，然后经过一个编码解码结构，然后通过仿真或者现有的地图信息构建真值label。最终优化的部分起到了明显的精度提升作用。


## 实验

### 数据集

使用了两个数据集，KITTI以及Cityscape

### 实验分析

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220421152251.png)

图六表示是否使用hallucination机制的定性对比，可以发现，如果不使用网络hallucination，那么由于前景遮挡，对应的深度无法预测。而使用了hallucination之后，将前景去除，这样就可以预测背景的深度信息。

表1表示各项改进的消融实验，可以发现直接预测语义类别相比预测RGB，IOU更高，而加上mask以及cls的编码，均得到了显著的精度提升。

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220421152509.png)

上图分为5列，每一列的左中右代表着不使用hallucination，使用hallucination, 以及使用refinement网络之后的结果。

其他实验详见论文。

## 总结与思考

BEV语义地图的构建，是针对背景的！因此在录制数据或运行建图算法时，前景动态目标的剔除十分重要，因为这些目标的遮挡会导致仅通过图像传感器无法很好地将透视视角转换成BEV视角，因为预测的深度会因为遮挡而不准。本文即为了解决此问题，所提出的方法也十分新颖，通过预处理将前景mask掉，这样网络就会只注意背景信息，再通过随机的mask，使网络训练可以得到对遮挡背景深度预测的能力。并且可以通过仿真以及Map的结合实现更高的效果。

综上，本文的2D-3D-BEV是通过预测深度实现的，而本文的重点内容在于通过hallucination解决前景目标导致的遮挡问题。

