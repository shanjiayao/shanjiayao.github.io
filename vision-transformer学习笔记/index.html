<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Vision Transformer学习笔记 - Jiayao&#39;s blog</title><meta name="Description" content="jiayao&#39;s blog"><meta property="og:title" content="Vision Transformer学习笔记" />
<meta property="og:description" content="自《Attention is all you need》之后，transformer先是在NLP领域大放异彩，继而近几年又在CV领域引领科研潮流，ViT、Swin Transformer几乎成了CV学者们耳熟能详的名词。本文是对Vision Transformer系列的学习总结，以ViT的框架为基础，梳理相关工作。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" /><meta property="og:image" content="http://shanjiayao.com/logo.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-02-21T19:07:13+00:00" />
<meta property="article:modified_time" content="2022-02-21T19:07:13+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://shanjiayao.com/logo.png"/>

<meta name="twitter:title" content="Vision Transformer学习笔记"/>
<meta name="twitter:description" content="自《Attention is all you need》之后，transformer先是在NLP领域大放异彩，继而近几年又在CV领域引领科研潮流，ViT、Swin Transformer几乎成了CV学者们耳熟能详的名词。本文是对Vision Transformer系列的学习总结，以ViT的框架为基础，梳理相关工作。"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" /><link rel="prev" href="http://shanjiayao.com/2021-%E8%BE%9B%E4%B8%91%E7%89%9B%E5%B9%B4/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Vision Transformer学习笔记",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/shanjiayao.com\/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0\/"
        },"genre": "posts","keywords": "Transformer","wordcount":  2631 ,
        "url": "http:\/\/shanjiayao.com\/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0\/","datePublished": "2022-02-21T19:07:13+00:00","dateModified": "2022-02-21T19:07:13+00:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "jiayao"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Jiayao&#39;s blog"></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/"> 主页 </a><a class="menu-item" href="/posts/"> 归档 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/projects/"> 项目 </a><a class="menu-item" href="/collections/"> 收集 </a><a class="menu-item" href="/about/"> 关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Jiayao&#39;s blog"></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/" title="">主页</a><a class="menu-item" href="/posts/" title="">归档</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/projects/" title="">项目</a><a class="menu-item" href="/collections/" title="">收集</a><a class="menu-item" href="/about/" title="">关于</a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animated flipInX">Vision Transformer学习笔记</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>jiayao</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="far fa-folder fa-fw"></i>深度学习</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2022-02-21">2022-02-21</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 2631 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 6 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#1-引言">1. 引言</a></li>
    <li><a href="#2-以vit为中心的研究风暴">2. 以ViT为中心的研究风暴</a>
      <ul>
        <li><a href="#21-cv应用transformer的难点">2.1 CV应用Transformer的难点</a></li>
        <li><a href="#22-vit概述">2.2 ViT概述</a>
          <ul>
            <li><a href="#vit的motivation是什么结构如何设计">ViT的Motivation是什么？结构如何设计？</a></li>
            <li><a href="#vit的结构范式与先前的transformer应用在cv中的工作有何不同">ViT的结构范式与先前的Transformer应用在CV中的工作有何不同？</a></li>
          </ul>
        </li>
        <li><a href="#23-以vit为中心梳理相关工作">2.3 以ViT为中心，梳理相关工作</a></li>
      </ul>
    </li>
    <li><a href="#相关文献">相关文献</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>自《<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreffer">Attention is all you need</a>》之后，transformer先是在NLP领域大放异彩，继而近几年又在CV领域引领科研潮流，ViT、Swin Transformer几乎成了CV学者们耳熟能详的名词。本文是对Vision Transformer系列的学习总结，以ViT的框架为基础，梳理相关工作。</p>
<h2 id="1-引言">1. 引言</h2>
<p>每一个研究热门的兴起，都值得人们思考其背后的意义。个人认为Transformer之于NLP（<em>Natural Language Processing</em>）以及CV（<em>Computer Vision</em>）领域，意义是极其重大的，除了其自身优异的架构性能，更多地，是因为其提供了一种可能，即以一种相对简单的方法打破两个领域之间的壁垒，也就是将NLP的先进经验不需要做太多修改便应用在CV领域。我们都知道近些年NLP领域的发展极其迅速，像是BERT等工作，<strong>无监督+大数据</strong>使其模型极其强大，固然需要大量的资源，但也足以覆盖NLP大多数问题了。而CV领域，目前的确有很多工作是在朝着无监督的方向发展，但是目前还谈不上比肩NLP的程度，私以为二者面对的任务不同，导致在模型设计上可能需要有很大的差异，要<strong>因材施教</strong>，所以要么设计出一种针对于CV任务的模型方案，要么就想办法复用NLP的先进经验，想办法将NLP的模型结构应用到CV领域。听起来貌似后者实现更容易一些，但是领域之间的鸿沟说大不大，说小也不小。说到这里，本文的主角Transformer就是后者方案的典型尝试，可以说Vision Transformer的工作直接打破了AlexNet以来CNN对CV领域的统治地位，虽然随着研究的不断迭代，CNN与Transformer谁更好用仍未可知，但并不妨碍我们探究Transformer的原理以及工作演变。</p>
<h2 id="2-以vit为中心的研究风暴">2. 以ViT为中心的研究风暴</h2>
<h3 id="21-cv应用transformer的难点">2.1 CV应用Transformer的难点</h3>
<p>关于Transformer的详细结构暂不在这里赘述，而纵观前几年的论文发表情况，Transformer先是在NLP领域中十分火热，并且总体来看，其性能上限随着模型参数量以及数据量的增长，仍保持增长状态，还未饱和。而近几年，则是有大量工作尝试在CV中复用Transformer的红利。按时间节点分析，2018年到2020年间，很多工作相继被提出，大多是结合CNN以及self attention，尝试在CV现有的框架中<strong>引入</strong>Transformer。它们或是直接替换CNN为self attention，或是将Transformer的输入从原始图片换成中间过程中的特征图，归根结底都是为了解决在CV与NLP之间的domain gap，概述有三：</p>
<ol>
<li><strong>输入数据的差异</strong>
<ol>
<li>NLP任务的输入是单词或语句，一般而言是一维的序列，且序列长度适中</li>
<li>CV任务一般输入是二维的图片，且图片的尺寸从早期的224×224(pixel)，逐渐发展到了近1000×1000(pixel)。</li>
<li>所以在数据上分析，如果将二维图片展开成一维向量，那么CV任务输入的数据尺寸将会远超过NLP任务。</li>
</ol>
</li>
<li><strong>计算量的差异</strong>
<ol>
<li>对于Transformer计算self attention来说，因为需要计算每个word之间的相关性，所以对应<strong>平方级别</strong>的时间复杂度。</li>
<li>NLP任务，长度适中的一维序列对应的计算量适中</li>
<li>CV任务则往往受限于与输入序列长度成平方级别的复杂度，导致应用Transformer较为困难，需要改变以往的模型范式。</li>
</ol>
</li>
<li><strong>Transformer与CNN的差异</strong>
<ol>
<li>CNN具有一些<em>inductive biases</em>，也叫归纳偏置，分别是locality以及translation equivariance。locality，局部性，是指图像相邻区域，特征相近。translation equivariance，平移等变性，是指CNN提取图片的特征针对平移操作是等变的。</li>
<li>Transformer可以关注global的信息，self attention可以计算输入序列中两两之间的相关性，所以相比于CNN，Transformer可以更好地捕捉长距离的信息。</li>
</ol>
</li>
</ol>
<p>因此，针对上述问题，在2020年，Google研发团队提出了ViT这篇工作，而自2020年到至今为止，大量的相关工作相继被提出，大多是以ViT为baseline，尝试优化其性能，后文也会梳理这些改动的工作。这里要提一下最近火热的Swin Transformer，其在CV各大任务上都完成了对CNN的反超，诸如检测、分割等下游任务，现在大多会应用Swin作为网络的主干，而Swin的框架也是改自ViT的，因此ViT的影响力可见一斑。</p>
<h3 id="22-vit概述">2.2 ViT概述</h3>
<p>明确了上述问题之后，我们来看一下ViT的解决方案，首先介绍一下ViT。</p>
<p><a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener noreffer">paper</a>  <a href="https://github.com/google-research/vision_transformer" target="_blank" rel="noopener noreffer">code</a></p>
<h4 id="vit的motivation是什么结构如何设计">ViT的Motivation是什么？结构如何设计？</h4>
<p>ViT受到了NLP领域中Transformer模型的可扩展性的启发，尝试将NLP中的Transformer直接应用在CV中，而不经过过多的修改，这样就可以实现在CV任务中训练一个包含几百亿乃至几千亿参数的大模型。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220222190859.png"
        data-srcset="https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220222190859.png, https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220222190859.png 1.5x, https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220222190859.png 2x"
        data-sizes="auto"
        alt="https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220222190859.png"
        title="https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220222190859.png" /></p>
<p>首先，为了解决计算量问题，ViT尝试对输入网络的图像<strong>划分patch</strong>，也就是将HxW的图像分成MxN个小块，然后将这些patches转换为一维的序列，这样有效地降低了输入Transformer的序列长度（1000x1000相比于14x14的差异）。处理好输入之后，ViT尝试直接应用NLP领域通用的Transformer结构，因为ViT针对分类任务，所以这里只用了Encoder，并且也尝试使用了class token来与每个patches的特征做信息交流，以提供用来分类的特征。
<em><strong>本质来讲，ViT的主旨就是想证明，NLP中的Transformer结构，在CV任务中，通过大规模的数据集进行训练，也可以达到比肩CNN的效果。</strong></em></p>
<h4 id="vit的结构范式与先前的transformer应用在cv中的工作有何不同">ViT的结构范式与先前的Transformer应用在CV中的工作有何不同？</h4>
<p>其实对图像分块的操作并不难想到，之前也有相关工作，但是很重要的一点是具体实现上，这也是为什么同样相似的工作，在google手上就可以达到这样出类拔萃效果的原因。更多的训练资源一定程度上影响了模型的性能，而这往往是大多数研究学者们不具备的。</p>
<p>而另一点不同，其实也是出发点的差异，ViT论文中通篇都在强调Transformer的可扩展性，强调NLP领域目前的成功，根本原因就是想通过ViT做一次尝试，大规模数据+直接使用NLP中的模型，能否取得一定的效果？答案是确定的。</p>
<h3 id="23-以vit为中心梳理相关工作">2.3 以ViT为中心，梳理相关工作</h3>
<h2 id="相关文献">相关文献</h2>
<p>A Survey on Vision Transformer</p>
<p>A Survey of Visual Transformers</p></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2022-02-21</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="Vision Transformer学习笔记" data-hashtags="Transformer"><i class="fab fa-twitter fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-hashtag="Transformer"><i class="fab fa-facebook-square fa-fw"></i></a><a href="javascript:void(0);" title="分享到 WhatsApp" data-sharer="whatsapp" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="Vision Transformer学习笔记" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="Vision Transformer学习笔记"><i data-svg-src="/lib/simple-icons/icons/line.min.svg"></i></a><a href="javascript:void(0);" title="分享到 Pocket" data-sharer="pocket" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fab fa-get-pocket fa-fw"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="Vision Transformer学习笔记"><i class="fab fa-weibo fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Myspace" data-sharer="myspace" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="Vision Transformer学习笔记" data-description=""><i data-svg-src="/lib/simple-icons/icons/myspace.min.svg"></i></a><a href="javascript:void(0);" title="分享到 Blogger" data-sharer="blogger" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="Vision Transformer学习笔记" data-description=""><i class="fab fa-blogger fa-fw"></i></a><a href="javascript:void(0);" title="分享到 Evernote" data-sharer="evernote" data-url="http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" data-title="Vision Transformer学习笔记"><i class="fab fa-evernote fa-fw"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/transformer/">Transformer</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/2021-%E8%BE%9B%E4%B8%91%E7%89%9B%E5%B9%B4/" class="prev" rel="prev" title="辛丑牛年的自己"><i class="fas fa-angle-left fa-fw"></i>辛丑牛年的自己</a></div>
</div>
<div id="comments"></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">年龄是时间的箭头，裹挟着我们，在匆匆的人世间，匆匆前行</div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2022</span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"同意","link":"了解更多","message":"本网站使用 Cookies 来改善您的浏览体验."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script><script type="text/javascript">
            window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}gtag('js', new Date());
            gtag('config', 'G-THB0NH45ZM', { 'anonymize_ip': true });
        </script><script type="text/javascript" src="https://www.googletagmanager.com/gtag/js?id=G-THB0NH45ZM" async></script></body>
</html>
