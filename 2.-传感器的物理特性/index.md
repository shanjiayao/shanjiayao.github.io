# 多传感器融合感知之用于感知的多传感器分析



本文记录了深蓝课程学习过程，主要是关于camera、lidar、radar等传感器的原理分析以及相关知识。

<!--more-->

## 传感器概述

常见的自动驾驶感知传感器，一般包括 lidar，camera，radar以及ultrasonic。

思考，为什么使用以上传感器来做感知任务？从人的角度分析，无非就是一些感官信息，触摸、看见、听见。所以从机器人的角度来看，上述传感器便能让机器人感知周边环境。

总的来看，激光使用激光束，camera使用可见光，radar使用毫米波，这三种都可以成为**电磁波**。
![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220317205109.png)

radar到camera到lidar，波长逐渐变小，我们都知道短波检测精度高，但是易受干扰，而长波传播距离远，抗干扰能力强。所以我们可以使用毫米波和激光波来互补。

此外，还有声波，比如超声波，这是一种机械波，其有一定透视能力，但是衰减快。其他形式的波还包括引力波、物质波等，但是均不太合适用作自动驾驶。

## 传感器原理分析

### camera

#### 成像原理

1. 针孔(pin-hole)相机为例，小孔成像原理
2. 相机是一种被动式传感器，依靠外界的光源（比如太阳）反射成像。因此对光照条件敏感。
3. 将三维映射为二维，丢失了尺度信息。
4. 清晰成像时需要满足像距、物距以及焦距之间的相对关系，假设焦距是f，像距是v，物距是u，那么应该满足，1/f = 1/u + 1/v，且像距应该大于一倍焦距小于二倍焦距，才可以呈现清晰的像。
5. 此外，相机传感器由于小孔成像的原理，存在一定的视场角，且视角由像距和传感器尺寸共同决定。此处可以通过相似三角形，求解水平和竖直的视角，如下图，视角之前的区域就是相机的盲区

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220318115518.png)

一般来讲，像距和焦距相差不大，在一些场景中可以认为像距等于焦距。并且焦距越大，视角越小，长焦看的远，短焦看得全，这也就是我们所说的长焦与短焦镜头的区别。

#### 成像过程

1. 景物在光源的照射下，产生的光学图像投射到相机中
2. **镜头**（LENS）
3. **图像传感器**，将光学信号转化为电信号，以及进一步通过AD转换器将模拟量转化为数字量。这里的光电转换传感器包含两种，分别是CCD以及CMOS，二者的差异在于信号的读出过程不同
	1. CCD仅能输出模拟电信号，集成度低，而 CMOS将图像信号放大器、信号读取电路、AD转换等集成到一块芯片上，集成度高
	2. CCD成本较高，工艺复杂，CMOS相对而言成本就低很多
	3. CMOS一般输出频率可以很高，500FPS以上频率输出的相机都是使用CMOS的
4. **DSP**，数字信号经过DSP(Digital Signal Processing)进行加工处理，DSP包括ISP(Image Signal Processor)以及JPEG编码器等组成，DSP的功能就是对原始的图像数据进行过滤和筛选，最终将处理好的信号传递下去
5. **IO**，负责与其他设备进行通信，目前主流的IO接口包括USB以及以太网等。

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220318131100.png)
#### 相机选型

对于感知任务来说，需要使用相机同时检测近距离和远距离的目标，且车辆覆盖的像素不少于30个，红绿灯覆盖的像素不少于10个，而在焦距固定时，相机是无法满足同时这个要求的，因此需要不同的焦距。使用长焦感知远距离，短焦感知近距离。

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220318134753.png)

常见的相机传感器供应商有 Sony、Onsemi（安森美）以及OmniVision（豪威）等。

相机传感器的一些核心指标包括像素尺寸、感光范围、输出帧率等。


#### 成像方式

分为两种，卷帘快门(rolloing shutter)和全局快门(global shutter)。
![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/Rolling-Shutter.gif)

自动驾驶场景中，一般使用rolling shutter，因为其制作工艺简单，成本低。

但是卷帘快门导致行与行之间成像时间有误差，进而存在两点问题
- 行之间时间戳不一致，一般取中间行的时间戳作为整张图片的时间戳。
- 行与行之间的成像有形变，这也叫做果冻效应，如下图

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/rolling-shutter.gif)

#### 小结

相机是一种被动式的传感器，依靠可见光感知目标，感知结果稠密且连续，但丢失尺度，视野范围依靠镜头焦距调节，卷帘快门出发，时间戳有误差，成本百元级别，量产方案成熟。


### lidar

#### 成像原理

首先，关于lidar的基本原理，参考以下视频 https://www.youtube.com/watch?v=NZKvf1cX

简要来说，激光雷达通过TOF原理，自己发射光波来测量环境中障碍物的距离，多线的激光雷达可以通过旋转式的扫描方式来获取周边环境的点云信息。常见的激光雷达不仅可以返回目标表面点相对于自身的距离，还可以测量表面材质、颜色等信息，作为额外的每个点的属性（intensity、timestamp、elongation等）。

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/video003.gif)

如上述简图所示，多个激光头并排旋转发射激光束，以构成周围环境的点云，其中，最上面激光头与最下面激光头的夹角叫做FOV，表示激光雷达竖直方向探测的角度范围。一般来讲， 可以通过修改激光雷达驱动文件，来配置线束之间的夹角，以达到下图的效果。

![](https://pictures-1309138036.cos.ap-nanjing.myqcloud.com/img/20220318211614.png)


此外，激光雷达有一个很显然的特性，就是距离远近的同一目标，其覆盖的点云数量不同，越远越少，因为角度决定密度，进而决定感知距离。

对于检测任务，理论上车辆表面不少于30个点，行人表面不少于10个点。

#### 常见的激光雷达类型以及核心参数

目前可以分为三类，分别是机械式、混合固态以及纯固态。

- **机械式**，通过并列排列若干个激光头来产生多个线束，并且使用旋转平台来完成360度的水平fov，通常来讲机械式雷达较为精密，应该避免磕碰，一旦机械结构发生形变，就会影响测量结果，所以其寿命通常不可控。常见品牌有velodyne、何塞、镭神、速腾等。
- **混合固态**，取消旋转平台，只用一个激光头，通过微振镜来产生不同的角度，但是并没有360度，水平和竖直都是确定的FOV，这种雷达使其有了过车规的可能，所以有一些无人驾驶公司会使用他们作为量产的解决方案。常见品牌有Innovution、DJI的livox等。
- **固态**，进一步去掉微振镜的结构，完全通过固态电路板来产生点云，成本会更低，但是现在技术还不是特别成熟，需要进一步发展。

通常来讲，我们会关注lidar的线束、波长、测量精度、探测距离、视角以及分辨率、旋转频率等参数。

#### 小结

Lidar是一种主动式传感器，通过自己发射激光束来对周边环境进行测距，生成的数据天然稀疏、但是测量距离精度高，且可以用来描述周边三维环境，其触发方式是逐点的，成像时间无误差，但是目前成本仍然是量产的一道阻碍。

### radar


