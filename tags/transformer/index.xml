<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Transformer - 标签 - Jiayao&#39;s blog</title>
        <link>http://shanjiayao.com/tags/transformer/</link>
        <description>Transformer - 标签 - Jiayao&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>shanjiayao97@gmail.com (jiayao)</managingEditor>
            <webMaster>shanjiayao97@gmail.com (jiayao)</webMaster><lastBuildDate>Wed, 27 Jan 2021 16:10:19 &#43;0000</lastBuildDate><atom:link href="http://shanjiayao.com/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>Transformer之self-attention机制</title>
    <link>http://shanjiayao.com/transformer%E4%B9%8Bself-attention/</link>
    <pubDate>Wed, 27 Jan 2021 16:10:19 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://shanjiayao.com/transformer%E4%B9%8Bself-attention/</guid>
    <description><![CDATA[<p>记录学习Transformer过程中的一些个人理解与思考</p>]]></description>
</item></channel>
</rss>
