<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Transformer - 标签 - Jiayao&#39;s blog</title>
        <link>http://shanjiayao.com/tags/transformer/</link>
        <description>Transformer - 标签 - Jiayao&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>shanjiayao97@gmail.com (jiayao)</managingEditor>
            <webMaster>shanjiayao97@gmail.com (jiayao)</webMaster><lastBuildDate>Mon, 21 Feb 2022 19:07:13 &#43;0000</lastBuildDate><atom:link href="http://shanjiayao.com/tags/transformer/" rel="self" type="application/rss+xml" /><item>
    <title>Vision Transformer学习笔记</title>
    <link>http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
    <pubDate>Mon, 21 Feb 2022 19:07:13 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://shanjiayao.com/vision-transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
    <description><![CDATA[<p>自《<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreffer">Attention is all you need</a>》之后，transformer先是在NLP领域大放异彩，继而近几年又在CV领域引领科研潮流，ViT、Swin Transformer几乎成了CV学者们耳熟能详的名词。本文是对Vision Transformer系列的学习总结，以ViT的框架为基础，梳理相关工作。</p>]]></description>
</item><item>
    <title>Transformer之self-attention机制</title>
    <link>http://shanjiayao.com/transformer%E4%B9%8Bself-attention/</link>
    <pubDate>Wed, 27 Jan 2021 16:10:19 &#43;0000</pubDate>
    <author>作者</author>
    <guid>http://shanjiayao.com/transformer%E4%B9%8Bself-attention/</guid>
    <description><![CDATA[<p>记录学习Transformer过程中的一些个人理解与思考</p>]]></description>
</item></channel>
</rss>
